---
title: 关于机器学习的基本概念
categories:
  - Python
date: 2018-09-15 20:41:08
updated: 2018-09-15 20:41:08
mathjax: true
tags: 
  - TensorFlow
  - Python
  - ML
---
纯属好奇心驱使的了解一下 TensorFlow 的相关知识，这 Google 有对应的课程可以了解真好，居然还真的需要一些袋鼠知识才能比较直观的理解呢。当然，理解基础知识，有利于后面对使用的快速上手和了解。 [课程地址](https://developers.google.com/machine-learning/crash-course/)

# 基本概念
按照课程上的解释，机器学习，会根据已有的**数据集 （我们称之为 样本）**来进行训练，得出一个模型。然后根据模块来对数据进行预测。

**数据集** 是一系列有 **特征**（ feature ) 与 **标签** ( label ) 的集合。

而需要进行预测的数据，则是，只有特征，没有标签。模型会根据特征，进行计算，得出标签的预测值。

## 回归与分类

**回归模型**可预测连续值。例如，回归模型做出的预测可回答如下问题：

* 加利福尼亚州一栋房产的价值是多少？
* 用户点击此广告的概率是多少？

**分类模型**可预测离散值。例如，分类模型做出的预测可回答如下问题：

* 某个指定电子邮件是垃圾邮件还是非垃圾邮件？
* 这是一张狗、猫还是仓鼠图片？

## 特征与权重
一个模型的输入可能有多个特征，每个特征的影响重要程度，可能并不一样，我们称之为特征的 **权重** 。对于 **线性回归**  还会有个初值。

## 训练与损失
预测肯定是有**偏差**的。  

**训练**模型指的是通过有标签的样本来学习（确定）所有权重和偏差的理想值。

在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为**经验风险最小化**。

**损失**是一个数值，表示对于单个样本而言模型预测的准确程度。

训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。

### 损失的评估

样本中的标签，与模型预测值之间的差异就是损失。 可以一个数学函数来对损失进行评估。

### 平方损失
线性回归常见的损失函数是 **平方损失 L2**。这种评估是用样本中的 **标签值 - 预测值** 进行平方计算。

### 均方误差MSE
**均方误差 (MSE)** 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：

$$
MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2
$$

## 迭代
预测都会有损失，我们的训练模型的**目的是找出平均损失最小的权重和偏差。**

迭代方法是一种广泛用于降低损失的方法，而且使用起来简单有效。

对于线性回归：

$$
y' = b + w_1x_1
$$

*b, w1* 我们可以随便选一个初值。这个时候，预测得到的值 y' 与 样本中的标签值 y 作为参数，让损失函数进行计算得到损失。如果我们以 **均方误差MSE** 来评估损失的话，那么每次训练，都会评估 MSE，生成新的 *w1* ，直到 MSE 不再变化，或者变化及其缓慢。这个时候我们可以说模型已经收敛（损失函数收敛）。

## 梯度下降法

在上面提到的迭代方法中，我们说学习的时候每次都会评估损失，生成新的**权重参数**，直到模型**收敛**。但是怎么生成参数，却没有进行解释。

**平均损失**的曲线与 $$w_1$$相关。只有一个 $$w_1$$ 值会让平均损失最小。

通过计算整个数据集中 *w1* 每个可能值的来找到收敛点这种方法效率太低。。

**梯度下降法**会根据平均损失的变化来确定生成 *w1* 的值。给 w1 一个初值，然后计算平均损失曲线在此处的 **梯度**。


**梯度** 是偏导数的矢量，其有方向和大小两个属性。梯度始终指向损失曲线中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加。

由于高数没学好，对于 **梯度** 不是很明白，但是没明白我心里就不踏实，所以去看了一下。

### 梯度

根据 [维基百科](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6) 解释：

* 在单变量的实值函数的情况（一元函数），梯度只是**导数**，或者，对于一个线性函数，也就是线的**斜率**。
* 梯度一词有时用于斜度，也就是一个曲面沿着给定方向的倾斜程度。可以通过取向量梯度和所研究的方向的内积来得到斜度。梯度的数值有时也被称为梯度。

大家自己看了。

## 学习速率（步长）

梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点（ w1 值）的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

如果步长过小，我们可能很久很久才能到达损失最低的位置；而步长过大，则我们可能永远也达不到损失最低的位置。

所以如果我们训练了一个模型后，就要进行一个测试认证模型的准确度。准确度太低的话，我们就要调整我们的模型**超参数了**。**超参数**是编程人员在机器学习算法中用于调整的旋钮。如**步长**。


## 随机梯度下降法

我们用梯度下降法来生成下一个 $$w_1$$ 的值。对于每个 $$w_1$$，我们需要一个样本集来计算损失曲线函数，然后求得梯度。这个样本集我们称之为 **批量**。如果 批量是以亿为单位的话，那么这个计算就非常的低效了。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法 (SGD) **将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法（小批量 SGD）**是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。
